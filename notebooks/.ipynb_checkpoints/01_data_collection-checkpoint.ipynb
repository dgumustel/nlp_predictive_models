{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abroad-supervisor",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "presidential-texas",
   "metadata": {},
   "source": [
    "### Project Introduction\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-basketball",
   "metadata": {},
   "source": [
    "### Subreddit selection\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "frank-council",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-memorial",
   "metadata": {},
   "source": [
    "---\n",
    "### Using Pushshift's API to pull data from subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-jumping",
   "metadata": {},
   "source": [
    "I knew that I would be pulling data from multiple different subreddits for this project so I created a function to streamline data pull requests. \n",
    "\n",
    "You can read more about Pushshift's API on this [GitHub page](https://github.com/pushshift/api). There is also a [YouTube video](https://www.youtube.com/watch?v=AcrjEWsMi_E) walkthrough of setting up this API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "collective-afternoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to pull data a specified number of times, from a specified subreddit, at a specified time\n",
    "def get_posts(pull_type, iters, subreddit, desired_time):\n",
    "    \n",
    "    # Define reddit's URL for requests\n",
    "    url = 'https://api.pushshift.io/reddit/search/' + str(pull_type)\n",
    "        \n",
    "    # Create empty master dataframe to fill\n",
    "    master_df = pd.DataFrame()\n",
    "    \n",
    "    # Loop through specified number \n",
    "    for i in range(iters):\n",
    "        # Set API parameters\n",
    "        params = {\n",
    "        'subreddit': subreddit,\n",
    "        'size': 100,\n",
    "        'before':desired_time}\n",
    "        \n",
    "        # Pull data\n",
    "        res = requests.get(url, params)\n",
    "        data = res.json()\n",
    "        posts = data['data']\n",
    "        df = pd.DataFrame(posts)\n",
    "        \n",
    "        # Concatenate data to master dataframe\n",
    "        frames = [df, master_df]\n",
    "        master_df = pd.concat(frames, axis=0, ignore_index=True)\n",
    "        \n",
    "        # Get time of oldest post in this data\n",
    "        # This resets the API parameters so that you pull older posts every iteration\n",
    "        desired_time = df['created_utc'].min()\n",
    "        print(f'Completed {i+1} iterations, {iters-i-1} iterations remaining')\n",
    "        \n",
    "        # Time delay so you don't get banned by Pushshift\n",
    "        time.sleep(60)\n",
    "    \n",
    "    # Return dataframe containing all collected posts\n",
    "    return master_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-platform",
   "metadata": {},
   "source": [
    "---\n",
    "### Pulling data from subreddits\n",
    "\n",
    "I went down two different routes for this project: first, create a model that can predict whether a post came from subreddit A or subreddit B; second, create a model that can predict whether a post from one subreddit came from year A or year B. Thus, I pulled data from 3 different subreddits but did four total pulls: one from [r/DMAcademy](https://www.reddit.com/r/DMAcademy/), one from [r/truezelda](https://www.reddit.com/r/truezelda/), one from [r/PoliticalDiscussion](https://www.reddit.com/r/PoliticalDiscussion/) in the year 2012, and one from r/PoliticalDiscussion in the year 2020. Each pull totaled 5,000 subreddit posts (post title and main text only, no comments), except for year 2020 pull from r/PoliticalDiscussion. For whatever reason, this subreddit gave me an error when trying to pull the last 100 posts, so I conceded to only gathering 4,900 from this year. \n",
    "\n",
    "I chose 5,000 posts to ensure that my models will be well-informed. It was recommended that my models be trained on 2,000 posts from each subreddit as a **minimum**, but I know that posts can be removed/deleted online, so I pulled well over the minimum recommended number to ensure that I would have enough posts to work with. However, 10,000 total posts is a lot for my models to crunch, so maybe aim for 3,000 or 4,000 per subreddit next time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-replica",
   "metadata": {},
   "source": [
    "### **Warning**\n",
    "Do not run any of the cells below unless you have ~4 hours to spare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-request",
   "metadata": {},
   "source": [
    "To start, let's pull 5,000 posts from the DMAcademy subreddit and store it in a dataframe. Remember, the `get_posts` function pulls 100 posts per iteration, so passing it 50 will produce 50 * 100 posts, or 5,000! Also, I passed `int(time.time())` to my `get_posts` function to pull the 5,000 most recent posts at the time of writing. When we move on to the political discussion posts, you'll see me use a specific time called [Unix or Epoch time](https://en.wikipedia.org/wiki/Unix_time) (formatted as number of seconds since 00:00:00 Jan 1, 1970, an arbitrary date) to pull posts from a specific date and time in 2012 and 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "pediatric-accused",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to investigate the missing links without waiting 100 minutes to pull data, uncomment the two lines below and run this cell\n",
    "\n",
    "# dmacademy_df = pd.read_csv('../data/dmacademy_df.csv')\n",
    "# truezelda_df = pd.read_csv('../data/truezelda.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "entertaining-conservative",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 1 iterations, 49 iterations remaining\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-a22898598ffa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdmacademy_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_posts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'submission'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'DMAcademy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-0e20ce37fcd3>\u001b[0m in \u001b[0;36mget_posts\u001b[1;34m(pull_type, iters, subreddit, desired_time)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;31m# Time delay so you don't get banned by Pushshift\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m# Return dataframe containing all collected posts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# dmacademy_df = get_posts('submission', 50, 'DMAcademy', int(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-helen",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inspect the dataframe\n",
    "dmacademy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-mother",
   "metadata": {},
   "outputs": [],
   "source": [
    "dmacademy_df['full_link'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-ridge",
   "metadata": {},
   "source": [
    "The printout above shows us the number of **unique reddit links** contained in our dataframe. This tells us that we didn't pull any duplicate posts, hooray! \n",
    "\n",
    "This all looks good, so now we can pull posts from r/truezelda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-hamilton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# truezelda_df = get_posts(50, 'truezelda', int(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-public",
   "metadata": {},
   "outputs": [],
   "source": [
    "truezelda_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-groove",
   "metadata": {},
   "outputs": [],
   "source": [
    "truezelda_df['id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-salon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are the missing ids nulls?\n",
    "truezelda_df['id'].isnull().sum()\n",
    "# Nope! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-young",
   "metadata": {},
   "source": [
    "For whatever reason, it looks like we may have pulled 4 duplicate posts. Since this is only .08% of our data from this subreddit, let's ignore it and use what we have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-going",
   "metadata": {},
   "source": [
    "---\n",
    "### Save data to .csv files\n",
    "Now that we've pulled the data needed for the first model, let's save it as `.csv`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-election",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set index=False to avoid creating an unnecessary index column\n",
    "dmacademy_df.to_csv('../data/dmacademy.csv', index=False)\n",
    "truezelda_df.to_csv('../data/truezelda.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-weather",
   "metadata": {},
   "source": [
    "---\n",
    "### Political discussion subreddit\n",
    "Now let's pull posts from r/PoliticalDiscussion. I'm pulling data from different years using an [Epoch time converter](https://www.epochconverter.com/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thousand-savannah",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define reddit's URL for requests\n",
    "\n",
    "url = 'https://api.pushshift.io/reddit/search/' + 'comment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-hello",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "'subreddit': 'PoliticalDiscussion',\n",
    "'size': 100,\n",
    "'before':1585630008}\n",
    "# Tuesday, March 31, 2020 4:46:48 AM\n",
    "# Pull data\n",
    "res = requests.get(url, params)\n",
    "data = res.json()\n",
    "posts = data['data']\n",
    "df = pd.DataFrame(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-ministry",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-bailey",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-waters",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['body'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-david",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['body']=='[removed]']['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-revelation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['body'].str.contains('Hello, /u/')]['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-queensland",
   "metadata": {},
   "outputs": [],
   "source": [
    "poli_dis_2020_df.loc[poli_dis_2020_df['body'].str.contains('I am a bot')]['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-letters",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to investigate the missing links without waiting 100 minutes to pull data, uncomment the two lines below and run this cell\n",
    "\n",
    "# poli_dis_2012_df = pd.read_csv('../data/poli_dis_2012.csv')\n",
    "# poli_dis_2020_df = pd.read_csv('../data/poli_dis_2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-german",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#poli_dis_2012_df = get_posts('comment', 50 , 'PoliticalDiscussion', 1333169208) \n",
    "# this time is Saturday, March 31, 2012 4:46:48 AM\n",
    "# or Friday, March 30, 2012 9:46:48 PM GMT-07:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-wheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "poli_dis_2012_df.loc[poli_dis_2012_df['body']=='[deleted]']['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-serbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "poli_dis_2012_df.loc[poli_dis_2012_df['body']=='[removed]']['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-power",
   "metadata": {},
   "outputs": [],
   "source": [
    "poli_dis_2012_df['body'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-charter",
   "metadata": {},
   "outputs": [],
   "source": [
    "poli_dis_2020_df['body'][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-camcorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "poli_dis_2012_df['body'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-branch",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#poli_dis_2020_df = get_posts('comment', 50, 'PoliticalDiscussion', 1585630008)\n",
    "# Tuesday, March 31, 2020 4:46:48 AM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-luther",
   "metadata": {},
   "outputs": [],
   "source": [
    "poli_dis_2012_df['year'] = '2012'\n",
    "poli_dis_2020_df['year'] = '2020'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-infrared",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(poli_dis_2012_df['id'].nunique())\n",
    "print(poli_dis_2020_df['id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-annual",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(poli_dis_2020_df['body'].nunique())\n",
    "print(poli_dis_2012_df['body'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-friendship",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(poli_dis_2012_df.shape)\n",
    "print(poli_dis_2020_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-conversion",
   "metadata": {},
   "source": [
    "# talk about data here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-presentation",
   "metadata": {},
   "source": [
    "---\n",
    "### Save data to .csv files\n",
    "Now that we've pulled the data needed for the second model, let's save it as `.csv`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-edgar",
   "metadata": {},
   "outputs": [],
   "source": [
    "poli_dis_2012_df.to_csv('../data/poli_dis_2012.csv', index=False)\n",
    "poli_dis_2020_df.to_csv('../data/poli_dis_2020.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-latitude",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-battery",
   "metadata": {},
   "source": [
    "### Pick up here!!!\n",
    "\n",
    "So far we've accomplished:\n",
    "* getting data - 200 subreddit posts so far (not comments)\n",
    "* count vectorizing the subreddit posts\n",
    "* passing data to TWO MODELS\n",
    "    * Bernoulli Naive Bayes model : when we have 0/1 variables.\n",
    "    * TFIDF multinomial naive bayes : when our variables are positive integers\n",
    "\n",
    "To do next:\n",
    "* get MORE DATA - source for pulling data on time delay: https://gist.github.com/tecoholic/1242694\n",
    "* get different types of data - try comments! try using titles alongside selftext! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-variable",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
