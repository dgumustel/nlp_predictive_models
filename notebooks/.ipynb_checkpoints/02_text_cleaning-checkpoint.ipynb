{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "suspected-provision",
   "metadata": {},
   "source": [
    "# Text Cleaning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-spare",
   "metadata": {},
   "source": [
    "### This notebook accomplishes the following:\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-holocaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-albania",
   "metadata": {},
   "source": [
    "---\n",
    "### Read in the data files\n",
    "We can read in the `.csv`s we created in the first notebook. We're working with four datasets total, so we'll do each step in the cleaning process four times. I create a list of my dataframes to loop through to reduce clutter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-maple",
   "metadata": {},
   "outputs": [],
   "source": [
    "dmacademy_df = pd.read_csv('../data/dmacademy.csv')\n",
    "truezelda_df = pd.read_csv('../data/truezelda.csv')\n",
    "poli_dis_2012_df = pd.read_csv('../data/poli_dis_2012.csv')\n",
    "poli_dis_2020_df = pd.read_csv('../data/poli_dis_2020.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-meaning",
   "metadata": {},
   "source": [
    "---\n",
    "### Checking for removed posts\n",
    "\n",
    "One column that caught my eye in these dataframe is the `removed_by_category` column. Each row of this column was found to contain one of six unique values: 'reddit', 'moderator', 'author', 'deleted', 'automod_filtered', or a null value. Rows where this column contains anything other than a null are posts that have been removed from the subreddit. This means the selftext only contains the word 'removed' or 'deleted'. \n",
    "\n",
    "Let's see how many rows in each dataframe might be posts that have been removed or deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-church",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect one of our dataframes\n",
    "dmacademy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-apparatus",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [dmacademy_df, truezelda_df, poli_dis_2012_df, poli_dis_2020_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-surfing",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in df_list:\n",
    "    try:\n",
    "        print(df['removed_by_category'].unique())\n",
    "        print(df[df['removed_by_category'].notnull()]['selftext'].unique())\n",
    "        print(df[df['removed_by_category'].notnull()]['selftext'].count(), '\\n')\n",
    "    except:\n",
    "        print(df[df['selftext']=='[deleted]']['selftext'].count())\n",
    "        print(df[df['selftext']=='[removed]']['selftext'].count())\n",
    "        print(df['selftext'].isnull().sum())\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-terrorism",
   "metadata": {},
   "source": [
    "For some odd reason, our 2012 political discussion dataset doesn't contain a `'removed_by_category'` column, and it doesn't contain rows that say 'deleted' or 'removed', but it does contain nulls.\n",
    "\n",
    "Based on this information, I decided to remove all rows where `selftext` is deleted, removed, or null. Unforunately, this means dropping hundreds of rows, but I intentionally pulled more data than I thought necessary to account for this possibility. \n",
    "\n",
    "Below, I print out examples of nulls, deleted, and removed posts in the DMAcademy data. All of these rows still have titles, but including the titles will give us the impression later that our classes are more balanced than they truly are, so I'd rather drop them, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-typing",
   "metadata": {},
   "outputs": [],
   "source": [
    "dmacademy_df[dmacademy_df['selftext'].isnull()][['selftext','title','removed_by_category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-perry",
   "metadata": {},
   "outputs": [],
   "source": [
    "dmacademy_df.loc[dmacademy_df['selftext']=='[deleted]'][['selftext','title','removed_by_category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-bahrain",
   "metadata": {},
   "outputs": [],
   "source": [
    "dmacademy_df.loc[dmacademy_df['selftext']=='[removed]'][['selftext','title','removed_by_category']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-claim",
   "metadata": {},
   "source": [
    "First, let's see jut how many rows we'll be dropping from each dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-success",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_names_list = ['dmacademy_df', 'truezelda_df', 'poli_dis_2012_df', 'poli_dis_2020_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-essence",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, dataframe in enumerate(df_list):\n",
    "    num_nulls = 0\n",
    "    num_nulls += len(dataframe.loc[dataframe['selftext']=='[deleted]'])\n",
    "    num_nulls += len(dataframe.loc[dataframe['selftext']=='[removed]'])\n",
    "    num_nulls += dataframe['selftext'].isnull().sum()\n",
    "    print(f'{df_names_list[i]} contains {num_nulls} empty posts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-generator",
   "metadata": {},
   "source": [
    "---\n",
    "### Drop rows with empty posts\n",
    "\n",
    "Now that we feel confident we've identified all the empty posts, let's drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-defense",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in df_list:\n",
    "    \n",
    "    # Drop rows where posts are nulls\n",
    "    df.dropna(axis=0, inplace=True, subset=['selftext'])\n",
    "\n",
    "    # Drop rows where posts were deleted\n",
    "    deleted_rows = df.loc[df['selftext']=='[deleted]'].index\n",
    "    df.drop(deleted_rows, inplace=True, axis=0)\n",
    "    \n",
    "    # Drop rows where posts were removed\n",
    "    removed_rows = df.loc[df['selftext']=='[removed]'].index\n",
    "    df.drop(removed_rows, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-dallas",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, dataframe in enumerate(df_list):\n",
    "    num_nulls = 0\n",
    "    num_nulls += len(dataframe.loc[dataframe['selftext']=='[deleted]'])\n",
    "    num_nulls += len(dataframe.loc[dataframe['selftext']=='[removed]'])\n",
    "    num_nulls += dataframe['selftext'].isnull().sum()\n",
    "    print(f'{df_names_list[i]} now contains {num_nulls} empty posts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-feelings",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in df_list:\n",
    "    print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-firewall",
   "metadata": {},
   "source": [
    "Our dataframes are significantly smaller now, but so much cleaner! Yay!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-harvard",
   "metadata": {},
   "source": [
    "---\n",
    "### Isolate the subreddit, title, and selftext columns \n",
    "After dropping empty posts, our dataframes still contain a lot of arbitrary data in all of those extra columns. Since this is a project on Natural Language Processing (NLP), I'm only going to use the title and main text (stored in the `title` and `selftext` columns, respectively) for every post. We can isolate these columns in new dataframes so that we aren't working with so much extraneous data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-fundamental",
   "metadata": {},
   "outputs": [],
   "source": [
    "dmacademy_df = dmacademy_df[['subreddit', 'title', 'selftext']].copy()\n",
    "truezelda_df = truezelda_df[['subreddit', 'title', 'selftext']].copy()\n",
    "poli_dis_2012_df = poli_dis_2012_df[['subreddit', 'title', 'selftext']].copy()\n",
    "poli_dis_2020_df = poli_dis_2020_df[['subreddit', 'title', 'selftext']].copy()\n",
    "\n",
    "    \n",
    "df_list = [dmacademy_df, truezelda_df, poli_dis_2012_df, poli_dis_2020_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-order",
   "metadata": {},
   "outputs": [],
   "source": [
    "dmacademy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-carpet",
   "metadata": {},
   "source": [
    "---\n",
    "### Inspect text for odd characters\n",
    "\n",
    "There are a lot of funky characters and strings in our text that we might want to consider removing before we tokenize and vectorize our data for model fitting. This includes typical relics like `\\n` and `&amp;`, but there's also some unseemly strings like `#x200B;`, as well as a lot of YouTube links. Some of these might be filtered out under the hood in our models, but I'm not an expert in NLP classification models so I'm going to do a bit of cleaning myself, or else I won't be able to say with confidence that this text data was properly processed. However, since I'm not expert, there is probably something I've missed.\n",
    "\n",
    "First, let's print a few posts from a single dataframe to skim for funky strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-charge",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for row in dmacademy_df['selftext'][1:2]:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-domestic",
   "metadata": {},
   "source": [
    "The post above showcases many of the examples I was describing before, including several YouTube links and one `&amp;#x200B;`. Let's use some regular expressions to replace these with empty strings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-surname",
   "metadata": {},
   "source": [
    "---\n",
    "### Remove digits, punctuation, and other small things "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-egyptian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I still have a lot to learn about regular expression and string processing, \n",
    "# so I relied on my classmate Amir and instructor John Hazard to help me with this code!\n",
    "\n",
    "\n",
    "# Iterate over each dataframe\n",
    "for dataframe in df_list:\n",
    "    # https://stackoverflow.com/questions/41719259/how-to-remove-numbers-from-string-terms-in-a-pandas-dataframe\n",
    "    dataframe.replace('\\d+', '', regex=True, inplace=True) # help from Amir! \n",
    "    dataframe.replace('&amp;', ' ', regex=True, inplace=True)\n",
    "    \n",
    "    # https://stackoverflow.com/questions/51994254/removing-url-from-a-column-in-pandas-dataframe/51994366\n",
    "    dataframe.replace('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', regex=True, inplace=True)\n",
    "    dataframe.replace('/', ' ', regex=True, inplace=True)\n",
    "    \n",
    "    # https://gist.github.com/smram/d6ded3c9028272360eb65bcab564a18a\n",
    "    dataframe.replace(to_replace=[r'\\\\t|\\\\n|\\\\r', '\\t|\\n|\\r'], value=[' ',' '], regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-plant",
   "metadata": {},
   "outputs": [],
   "source": [
    "dmacademy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-avenue",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for row in dmacademy_df['selftext'][1:2]:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-atmosphere",
   "metadata": {},
   "source": [
    "We still have a lot of odd punctuation, brackets, and parentheses, but these will be removed by our tokenizer later. This processing job wasn't perfect, there's still a `#xB;` in there, but I'm satisfied with having removed the YouTube links, digits, and ampersands, and having replaced all `/`s with a space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-stadium",
   "metadata": {},
   "source": [
    "---\n",
    "### Check for null values\n",
    "Some posts are title-only, meaning the selftext column contains a null value. We don't want to pass any nulls to our models, so we can replace nulls with empty strings. \n",
    "\n",
    "Again, this is why it's important that we chose text-heavy subreddits in our data collection. If we chose subreddits where most posts contained images or links to other websites, then we would have more null values and less text, leading to our models being less informed. I'm comfortable with the small amount of null values here, so I don't need to change to a more text-rich subreddit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-horizon",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, dataframe in enumerate(df_list):\n",
    "    print(f'Nulls in {df_names_list[i]} dataframe\\n', dataframe.isnull().sum(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-update",
   "metadata": {},
   "source": [
    "We can even see which of our subreddits contains most of the nulls. Looks like the posts from r/truezelda and from r/PoliticalDiscussion in the year 2012 contain the most nulls. I'm a little concerned to see 701 nulls in one dataframe, because it means our classes will be more imbalanced than I would have liked, but that's why we pulled 5,000 posts in the first place! We prepared for this imbalance by collecting more than the minimum recommendation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-occurrence",
   "metadata": {},
   "source": [
    "Still, I'd rather not drop them in case the titles still contain useful text. Instead of dropping these rows, we can replace the null values with a string containing a single space. At first, I tried replacing them with empty strings (no space), but they get turned back into nulls when you read them into the other notebooks. Our classification models can't process null values, so I'm replacing their entries with a single space to prevent our models from breaking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outdoor-topic",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataframe in df_list:\n",
    "    dataframe.replace(np.nan, ' ', regex=True, inplace=True)\n",
    "    print(dataframe.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-compact",
   "metadata": {},
   "source": [
    "--- \n",
    "### Check for removed or deleted posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-television",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-substitute",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CountVectorizer(stop_words='english').get_stop_words())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grand-cleaning",
   "metadata": {},
   "source": [
    "---\n",
    "### Map our target variable to integer values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-amendment",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-civilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-grounds",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this step is optional\n",
    "#text_df['subreddit'] = text_df['subreddit'].map({'truezelda':1,'DMAcademy':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-window",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-pencil",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "sonic-content",
   "metadata": {},
   "source": [
    "---\n",
    "### Save processed data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-pharmacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "dmacademy_df.to_csv('../data/clean_dmacademy.csv', index=False)\n",
    "truezelda_df.to_csv('../data/clean_truezelda.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-damage",
   "metadata": {},
   "source": [
    "Now we're ready to move on to notebook 3, creating a model to predict which subreddit a post came from!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-colonial",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
